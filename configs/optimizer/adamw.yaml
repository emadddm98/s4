# _target_: torch.optim.AdamW
_name_: adamw
lr: 0.002 #0.001 #2.5e-4 #3e-4 #0.0005 #0.001 # Initial learning rate
weight_decay: 0.01 #0.02 #0.01 #0.00 # Weight decay
betas: [0.9, 0.999] #[0.9, 0.999] # Beta parameters for AdamW
eps: 1e-8 #1e-8 # Epsilon for numerical stability
