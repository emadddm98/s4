# @package _global_
train:
  interval: epoch
  monitor: val/loss #val/accuracy #??? # must be specified
scheduler:
  # _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _name_: plateau
  mode: ${train.mode} #min #max #${train.mode} # Which metric to monitor
  factor: 0.5 #0.4 #0.5 #0.2  # Decay factor when ReduceLROnPlateau is used
  patience: 3 #5 #3 #20
  threshold: 0.001 #0 #0.001 # Threshold for measuring new optimum #wasn't there
  cooldown: 1 #2 #0 all experiments # Cooldown period after learning rate has been reduced #wasn't there
  min_lr: 5e-6 #1.0e-6 #0.0  # Minimum learning rate during annealing
